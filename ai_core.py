"""
æ³•å¾‹è€ƒè©¦è¼”åŠ©ç³»çµ± - AI æ ¸å¿ƒæ¨¡çµ„ï¼ˆå®Œæ•´ç‰ˆ + é¢¨æ ¼æ”¯æ´ + è¦–è¦ºåŒ–ï¼‰
è™•ç† Gemini APIã€Pinecone å‘é‡æœå°‹ã€RAG å•ç­”ã€å¿ƒæ™ºåœ–ç”Ÿæˆ
"""

import google.generativeai as genai
from pinecone import Pinecone, ServerlessSpec
from config import Config
from ai_cache import AICache
import time
import json
import re
from typing import List, Dict, Optional

class AICore:
    """AI æ ¸å¿ƒè™•ç†é¡åˆ¥"""
    
    def __init__(self):
        """åˆå§‹åŒ– AI æœå‹™"""
        try:
            # é…ç½® Gemini
            genai.configure(api_key=Config.GEMINI_API_KEY)
            
            # åˆå§‹åŒ–æ¨¡å‹
            self.model = genai.GenerativeModel(Config.GEMINI_MODEL)
            self.embedding_model = Config.EMBEDDING_MODEL
            
            # åˆå§‹åŒ– Pinecone
            self.pc = Pinecone(api_key=Config.PINECONE_API_KEY)
            self.index = self._init_pinecone_index()
            
            # åˆå§‹åŒ– AI å¿«å–
            self.cache = AICache(cache_file="ai_cache.json", max_age_hours=24)
            
            print("âœ… AI æœå‹™åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ AI åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    def _init_pinecone_index(self):
        """åˆå§‹åŒ– Pinecone ç´¢å¼•"""
        index_name = Config.PINECONE_INDEX_NAME
        
        # æª¢æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        existing_indexes = [idx['name'] for idx in self.pc.list_indexes()]
        
        if index_name not in existing_indexes:
            print(f"ğŸ“¦ å»ºç«‹æ–°çš„ Pinecone ç´¢å¼•: {index_name}")
            self.pc.create_index(
                name=index_name,
                dimension=Config.EMBEDDING_DIMENSION,
                metric='cosine',
                spec=ServerlessSpec(
                    cloud='aws',
                    region='us-east-1'
                )
            )
            # ç­‰å¾…ç´¢å¼•æº–å‚™å°±ç·’
            print("â³ ç­‰å¾…ç´¢å¼•åˆå§‹åŒ–...")
            time.sleep(10)
        
        # é€£æ¥åˆ°ç´¢å¼•ä¸¦è¿”å›
        index = self.pc.Index(index_name)
        print(f"âœ… Pinecone ç´¢å¼•å·²é€£æ¥: {index_name}")
        return index
    
    def generate_embedding(self, text: str, task_type: str = "retrieval_document") -> List[float]:
        """ç”Ÿæˆæ–‡å­—åµŒå…¥å‘é‡
        
        Args:
            text: è¦ç”Ÿæˆå‘é‡çš„æ–‡å­—
            task_type: ä»»å‹™é¡å‹
                - "retrieval_document": ç”¨æ–¼å„²å­˜æ–‡æª”ï¼ˆé è¨­ï¼‰
                - "retrieval_query": ç”¨æ–¼æœå°‹æŸ¥è©¢
                - "semantic_similarity": ç”¨æ–¼èªç¾©ç›¸ä¼¼åº¦æ¯”è¼ƒ
        """
        try:
            result = genai.embed_content(
                model=Config.EMBEDDING_MODEL,
                content=text,
                task_type=task_type
            )
            return result['embedding']
        except Exception as e:
            print(f"âŒ åµŒå…¥ç”ŸæˆéŒ¯èª¤: {e}")
            return [0.0] * Config.EMBEDDING_DIMENSION
    
    def add_to_knowledge_base(self, content: str, metadata: dict) -> bool:
        """å°‡å…§å®¹åŠ å…¥çŸ¥è­˜åº«"""
        try:
            # ç”ŸæˆåµŒå…¥å‘é‡
            embedding = self.generate_embedding(content)
            
            # ç”Ÿæˆå”¯ä¸€ ID
            vector_id = f"{metadata.get('type', 'note')}_{metadata.get('note_id', 'unknown')}_{hash(content) % 10000}"
            
            # ç²¾ç°¡ metadataï¼Œåªä¿ç•™å¿…è¦è³‡è¨Šï¼ˆé¿å…è¶…é 40KB é™åˆ¶ï¼‰
            compact_metadata = {
                'note_id': metadata.get('note_id', ''),
                'user_id': metadata.get('user_id', ''),
                'title': metadata.get('title', '')[:200],  # é™åˆ¶æ¨™é¡Œé•·åº¦
                'category': metadata.get('category', ''),
                'difficulty': metadata.get('difficulty', ''),
                'type': metadata.get('type', 'note'),
                # åªå„²å­˜æ¨™ç±¤åˆ—è¡¨ï¼Œä¸å„²å­˜å®Œæ•´å…§å®¹
                'tags': metadata.get('tags', [])[:10] if isinstance(metadata.get('tags'), list) else [],
                # å„²å­˜å…§å®¹é è¦½ï¼ˆå‰ 500 å­—ï¼‰ï¼Œä¸å„²å­˜å®Œæ•´å…§å®¹
                'content': content[:500] + '...' if len(content) > 500 else content,
                # å®Œæ•´å…§å®¹å„²å­˜åœ¨ full_contentï¼ˆä½†ä¹Ÿè¦é™åˆ¶å¤§å°ï¼‰
                'full_content': content[:5000] + '...' if len(content) > 5000 else content
            }
            
            # ä¸Šå‚³åˆ° Pinecone
            self.index.upsert(
                vectors=[{
                    'id': vector_id,
                    'values': embedding,
                    'metadata': compact_metadata
                }]
            )
            
            print(f"âœ… å·²åŠ å…¥çŸ¥è­˜åº«: {metadata.get('title', 'Unknown')}")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ å…¥çŸ¥è­˜åº«å¤±æ•—: {e}")
            return False
    
    def delete_from_knowledge_base(self, note_id: str) -> bool:
        """å¾çŸ¥è­˜åº«åˆªé™¤å…§å®¹ï¼ˆä½¿ç”¨ metadata éæ¿¾ï¼‰"""
        try:
            # ä½¿ç”¨ metadata éæ¿¾åˆªé™¤ï¼ˆå› ç‚ºå„²å­˜æ™‚çš„ ID æ ¼å¼æ˜¯ note_{note_id}_{hash}ï¼‰
            self.index.delete(filter={"note_id": note_id})
            print(f"âœ… å·²å¾çŸ¥è­˜åº«åˆªé™¤: {note_id}")
            return True
        except Exception as e:
            # å¦‚æœ metadata éæ¿¾å¤±æ•—ï¼Œå˜—è©¦ç›´æ¥åˆªé™¤ï¼ˆå‘å¾Œç›¸å®¹ï¼‰
            try:
                self.index.delete(ids=[note_id])
                print(f"âœ… å·²å¾çŸ¥è­˜åº«åˆªé™¤ï¼ˆä½¿ç”¨ IDï¼‰: {note_id}")
                return True
            except Exception as e2:
                print(f"âŒ å¾çŸ¥è­˜åº«åˆªé™¤å¤±æ•—: {e}, {e2}")
                return False
    
    def search_knowledge_base(self, query: str, top_k: int = None, 
                             category: str = None) -> List[Dict]:
        """æœå°‹çŸ¥è­˜åº«"""
        if top_k is None:
            top_k = Config.MAX_SEARCH_RESULTS
        
        try:
            # æœå°‹æ™‚ä½¿ç”¨ retrieval_query task_typeï¼ˆèˆ‡å„²å­˜æ™‚çš„ retrieval_document é…å°ï¼‰
            query_embedding = self.generate_embedding(query, task_type="retrieval_query")
            filter_dict = {'category': category} if category else None
            
            results = self.index.query(
                vector=query_embedding,
                top_k=top_k * 3,  # å–æ›´å¤šçµæœä¾†ç¯©é¸
                include_metadata=True,
                filter=filter_dict
            )
            
            # é™¤éŒ¯ï¼šé¡¯ç¤ºæ‰€æœ‰çµæœçš„åˆ†æ•¸
            print(f"[Search] Pinecone returned {len(results.get('matches', []))} results")
            for i, match in enumerate(results.get('matches', [])[:5]):
                print(f"  Result {i+1}: Score {match['score']:.3f} - {match['metadata'].get('title', 'N/A')}")
            
            filtered_results = []
            for match in results['matches']:
                # æé«˜é–¾å€¼åˆ° 0.85ï¼Œæ¥µåº¦åš´æ ¼ï¼Œå¯§ç¼ºå‹¿æ¿«
                if match['score'] >= 0.85:
                    filtered_results.append({
                        'score': match['score'],
                        'content': match['metadata'].get('full_content', 
                                                        match['metadata'].get('content', '')),
                        'metadata': match['metadata']
                    })
            
            filtered_results = filtered_results[:top_k]
            
            if filtered_results:
                print(f"[OK] Found {len(filtered_results)} highly relevant results (threshold >= 0.85)")
            else:
                print(f"[Warning] No results with relevance >= 0.85")
                # é¡¯ç¤ºæœ€é«˜åˆ†æ•¸ä¾›è¨ºæ–·
                if results.get('matches'):
                    max_score = results['matches'][0]['score']
                    print(f"   Max score: {max_score:.3f} (below threshold)")
                    print(f"   Suggestion: Run rebuild_pinecone_index.py")
            
            return filtered_results
            
        except Exception as e:
            print(f"[Error] Search failed: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def generate_ai_notes(self, content: str, note_type: str = "é‡é»æ•´ç†", 
                         style_instruction: str = "") -> str:
        """ä½¿ç”¨ AI ç”Ÿæˆç­†è¨˜ï¼ˆæ”¯æ´è‡ªè¨‚é¢¨æ ¼ï¼‰"""
        
        base_prompts = {
            "é‡é»æ•´ç†": f"""è«‹å°‡ä»¥ä¸‹æ³•å¾‹å…§å®¹æ•´ç†æˆç­†è¨˜ã€‚

é¢¨æ ¼è¦æ±‚ï¼š{style_instruction}

åŸå§‹å…§å®¹ï¼š
{content}

**é‡è¦æŒ‡ç¤º**ï¼š
1. ç›´æ¥è¼¸å‡ºç­†è¨˜å…§å®¹ï¼Œä¸è¦æœ‰ä»»ä½•é–‹å ´ç™½ï¼ˆä¾‹å¦‚ï¼šã€Œå¥½çš„ã€ã€ã€Œä»¥ä¸‹æ˜¯ã€ã€ã€Œå·²ç‚ºæ‚¨æ•´ç†ã€ç­‰ï¼‰
2. ä¸è¦æœ‰çµå°¾ç¥ç¦æˆ–é¼“å‹µçš„è©±
3. ä¸è¦èªªæ˜ä½ åšäº†ä»€éº¼ï¼Œç›´æ¥çµ¦å‡ºçµæœ
4. ç”¨ç¹é«”ä¸­æ–‡
5. ç«‹å³å¾ç­†è¨˜æ¨™é¡Œæˆ–å…§å®¹é–‹å§‹""",

            "è€ƒé»åˆ†æ": f"""è«‹åˆ†æä»¥ä¸‹å…§å®¹çš„é‡è¦è€ƒé»ã€‚

é¢¨æ ¼è¦æ±‚ï¼š{style_instruction}

åŸå§‹å…§å®¹ï¼š
{content}

**é‡è¦æŒ‡ç¤º**ï¼š
1. ç›´æ¥è¼¸å‡ºè€ƒé»åˆ†æå…§å®¹ï¼Œä¸è¦æœ‰ä»»ä½•é–‹å ´ç™½ï¼ˆä¾‹å¦‚ï¼šã€Œå¥½çš„ã€ã€ã€Œä»¥ä¸‹æ˜¯ã€ã€ã€Œå·²ç‚ºæ‚¨æ•´ç†ã€ç­‰ï¼‰
2. ä¸è¦æœ‰çµå°¾ç¥ç¦æˆ–é¼“å‹µçš„è©±
3. ä¸è¦èªªæ˜ä½ åšäº†ä»€éº¼ï¼Œç›´æ¥çµ¦å‡ºçµæœ
4. ç”¨ç¹é«”ä¸­æ–‡
5. ç«‹å³å¾è€ƒé»å…§å®¹é–‹å§‹""",

            "æ¡ˆä¾‹è§£æ": f"""è«‹é‡å°ä»¥ä¸‹å…§å®¹æä¾›æ¡ˆä¾‹è§£æã€‚

é¢¨æ ¼è¦æ±‚ï¼š{style_instruction}

åŸå§‹å…§å®¹ï¼š
{content}

**é‡è¦æŒ‡ç¤º**ï¼š
1. ç›´æ¥è¼¸å‡ºæ¡ˆä¾‹è§£æå…§å®¹ï¼Œä¸è¦æœ‰ä»»ä½•é–‹å ´ç™½ï¼ˆä¾‹å¦‚ï¼šã€Œå¥½çš„ã€ã€ã€Œä»¥ä¸‹æ˜¯ã€ã€ã€Œå·²ç‚ºæ‚¨æ•´ç†ã€ç­‰ï¼‰
2. ä¸è¦æœ‰çµå°¾ç¥ç¦æˆ–é¼“å‹µçš„è©±
3. ä¸è¦èªªæ˜ä½ åšäº†ä»€éº¼ï¼Œç›´æ¥çµ¦å‡ºçµæœ
4. ç”¨ç¹é«”ä¸­æ–‡
5. ç«‹å³å¾æ¡ˆä¾‹å…§å®¹é–‹å§‹"""
        }
        
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                prompt = base_prompts.get(note_type, base_prompts["é‡é»æ•´ç†"])
                print(f"ğŸ¤– æ­£åœ¨ç”Ÿæˆ{note_type}... (å˜—è©¦ {attempt + 1}/{max_retries})")
                
                # ç”Ÿæˆå…§å®¹ï¼ˆGemini API ä¸æ”¯æ´ request_optionsï¼‰
                response = self.model.generate_content(prompt)
                
                print(f"âœ… {note_type}ç”Ÿæˆå®Œæˆ")
                
                # å¾Œè™•ç†ï¼šç§»é™¤å¸¸è¦‹çš„é–‹å ´ç™½æ¨¡å¼
                result = response.text.strip()
                
                # ç§»é™¤å¸¸è¦‹çš„é–‹å ´ç™½å¥å­ï¼ˆåŒ…å« OCR å°ˆç”¨ï¼‰
                preamble_patterns = [
                    r'^å¥½çš„[ï¼Œ,ã€‚ï¼!]*',
                    r'^ä»¥ä¸‹æ˜¯.*?[ï¼š:]\s*',
                    r'^å·²ç‚ºæ‚¨æ•´ç†.*?[ï¼š:]\s*',
                    r'^é€™ä»½.*?ç­†è¨˜.*?å¦‚ä¸‹.*?[ï¼š:]\s*',
                    r'^æ ¹æ“š.*?å…§å®¹.*?[ï¼š:]\s*',
                    r'^.*?æ•´ç†å¦‚ä¸‹.*?[ï¼š:]\s*',
                    r'^.*?å·²æ•´ç†.*?[ï¼š:]\s*',
                    r'^è®“æˆ‘.*?[ï¼š:]\s*',
                    r'^æˆ‘å°‡.*?[ï¼š:]\s*',
                    r'^ä»¥ä¸‹.*?Markdown.*?[ï¼š:]\s*',
                    # OCR å°ˆç”¨æ¨¡å¼
                    r'^ä»¥ä¸‹æ˜¯æ ¹æ“š.*?OCR.*?è¾¨è­˜.*?[ï¼š:ï¼Œ,ã€‚]*\s*',
                    r'^æ ¹æ“š.*?OCR.*?æ–‡å­—.*?[ï¼š:ï¼Œ,ã€‚]*\s*',
                    r'^ä»¥ä¸‹æ˜¯.*?OCR.*?å…§å®¹.*?[ï¼š:ï¼Œ,ã€‚]*\s*',
                    r'^.*?æ•´ç†è€Œæˆçš„.*?å…§å®¹.*?[ï¼š:ï¼Œ,ã€‚]*\s*',
                    r'^.*?é€šé †.*?å®Œæ•´.*?æ ¼å¼åŒ–.*?[ï¼š:ï¼Œ,ã€‚]*\s*',
                ]
                
                for pattern in preamble_patterns:
                    result = re.sub(pattern, '', result, flags=re.MULTILINE)
                
                # ç§»é™¤é–‹é ­çš„ç©ºè¡Œ
                result = result.lstrip('\n')
                
                return result
                
            except Exception as e:
                error_str = str(e)
                if "504" in error_str or "timeout" in error_str.lower():
                    if attempt < max_retries - 1:
                        print(f"âš ï¸ è«‹æ±‚è¶…æ™‚ï¼Œ{retry_delay}ç§’å¾Œé‡è©¦...")
                        time.sleep(retry_delay)
                        retry_delay *= 2  # æŒ‡æ•¸é€€é¿
                        continue
                    else:
                        return f"ç”Ÿæˆç­†è¨˜æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼šè«‹æ±‚è¶…æ™‚ã€‚è«‹å˜—è©¦ï¼š\n1. æ¸›å°‘è¼¸å…¥å…§å®¹é•·åº¦\n2. ç¨å¾Œå†è©¦\n3. æª¢æŸ¥ç¶²è·¯é€£ç·š"
                else:
                    error_msg = f"ç”Ÿæˆç­†è¨˜æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
                    print(f"âŒ {error_msg}")
                    return error_msg
    
    def answer_question_with_rag(self, question: str, 
                                 context_results: List[Dict] = None) -> str:
        """ä½¿ç”¨ RAG å›ç­”å•é¡Œ"""
        if context_results is None:
            context_results = self.search_knowledge_base(question, top_k=3)
        
        if context_results:
            context = "\n\n".join([
                f"ã€åƒè€ƒè³‡æ–™ {i+1}ã€‘ï¼ˆç›¸é—œåº¦: {r['score']:.1%})\n"
                f"æ¨™é¡Œ: {r['metadata'].get('title', 'ç„¡æ¨™é¡Œ')}\n"
                f"å…§å®¹: {r['content']}"
                for i, r in enumerate(context_results)
            ])
        else:
            context = "ï¼ˆçŸ¥è­˜åº«ä¸­æš«ç„¡ç›¸é—œè³‡æ–™ï¼‰"
        
        prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ³•å¾‹è€ƒè©¦è¼”å°è€å¸«ã€‚è«‹æ ¹æ“šçŸ¥è­˜åº«å…§å®¹å›ç­”å­¸ç”Ÿçš„å•é¡Œã€‚

ã€çŸ¥è­˜åº«åƒè€ƒè³‡æ–™ã€‘
{context}

ã€å­¸ç”Ÿå•é¡Œã€‘
{question}

è«‹ä»¥å°ˆæ¥­ä½†æ˜“æ‡‚çš„æ–¹å¼å›ç­”ï¼ŒåŒ…å«ï¼š
1. ç›´æ¥å›ç­”å•é¡Œçš„æ ¸å¿ƒ
2. å¼•ç”¨ç›¸é—œæ³•æ¢ï¼ˆå¦‚æœæœ‰ï¼‰
3. èªªæ˜åœ¨å¯¦å‹™ä¸Šçš„æ‡‰ç”¨
4. è£œå……æ³¨æ„äº‹é …

å¦‚æœçŸ¥è­˜åº«å…§å®¹ä¸è¶³ï¼Œè«‹å…ˆèªªæ˜ã€ŒçŸ¥è­˜åº«ä¸­ç›¸é—œè³‡æ–™æœ‰é™ã€ï¼Œç„¶å¾ŒåŸºæ–¼ä¸€èˆ¬æ³•å¾‹åŸå‰‡æä¾›å»ºè­°ã€‚
åªè¼¸å‡ºå›ç­”å…§å®¹ï¼Œä¸è¦æœ‰é–‹å ´ç™½æˆ–çµå°¾ç¥ç¦ã€‚ç”¨ç¹é«”ä¸­æ–‡ã€‚"""
        
        try:
            print(f"ğŸ¤– æ­£åœ¨æ€è€ƒç­”æ¡ˆ...")
            response = self.model.generate_content(prompt)
            print(f"âœ… å›ç­”å®Œæˆ")
            return response.text
            
        except Exception as e:
            error_msg = f"å›ç­”å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
            print(f"âŒ {error_msg}")
            return error_msg
    
    def generate_quiz_question(self, content: str = None, category: str = None) -> Dict:
        """ç”Ÿæˆæ³•å¾‹çˆ­é»é¸æ“‡é¡Œï¼ˆJSON æ ¼å¼ï¼‰"""
        
        source_content = ""
        if content:
            source_content = f"åŸºæ–¼ä»¥ä¸‹ç­†è¨˜å…§å®¹å‡ºé¡Œï¼š\n{content}"
        else:
            source_content = f"è«‹å‡ºä¸€é¡Œé—œæ–¼ã€Œ{category if category else 'æ³•å¾‹'}ã€çš„çˆ­é»é¸æ“‡é¡Œã€‚"

        prompt = f"""ä½ æ˜¯ä¸€å€‹æ³•å¾‹å‡ºé¡Œè€å¸«ã€‚è«‹{source_content}

è«‹ç”Ÿæˆä¸€å€‹å–®é¸é¡Œï¼Œæ ¼å¼å¿…é ˆæ˜¯åˆæ³•çš„ JSONï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
1. question: é¡Œç›®æ•˜è¿°
2. options: ä¸€å€‹åŒ…å« 4 å€‹é¸é …å­—ä¸²çš„é™£åˆ— (ä¸éœ€è¦ A, B, C, D å‰ç¶´)
3. answer_index: æ­£ç¢ºç­”æ¡ˆçš„ç´¢å¼• (0-3)
4. explanation: è©³ç´°è§£æ

ç¯„ä¾‹æ ¼å¼ï¼š
{{
  "question": "é—œæ–¼...ä¸‹åˆ—ä½•è€…æ­£ç¢ºï¼Ÿ",
  "options": ["é¸é …ä¸€", "é¸é …äºŒ", "é¸é …ä¸‰", "é¸é …å››"],
  "answer_index": 1,
  "explanation": "å› ç‚º..."
}}

è«‹åªè¼¸å‡º JSONï¼Œä¸è¦æœ‰ Markdown code block æ¨™è¨˜æˆ–å…¶ä»–æ–‡å­—ã€‚ç”¨ç¹é«”ä¸­æ–‡ã€‚"""

        try:
            print(f"ğŸ¤– æ­£åœ¨ç”Ÿæˆé¡Œç›®...")
            response = self.model.generate_content(prompt)
            text = response.text.strip()
            
            # å˜—è©¦è§£æ JSON
            match = re.search(r'\{.*\}', text, re.DOTALL)
            if match:
                json_str = match.group(0)
                quiz_data = json.loads(json_str)
                print(f"âœ… é¡Œç›®ç”Ÿæˆå®Œæˆ")
                return quiz_data
            else:
                print(f"âš ï¸ ç„¡æ³•è§£æ JSONï¼ŒåŸå§‹å›æ‡‰ï¼š{text}")
                return {
                    "question": "é¡Œç›®ç”Ÿæˆå¤±æ•—ï¼Œè«‹é‡è©¦",
                    "options": ["éŒ¯èª¤", "éŒ¯èª¤", "éŒ¯èª¤", "éŒ¯èª¤"],
                    "answer_index": 0,
                    "explanation": f"ç„¡æ³•è§£æ AI å›æ‡‰: {text}"
                }
                
        except Exception as e:
            error_msg = f"ç”Ÿæˆé¡Œç›®éŒ¯èª¤: {e}"
            print(f"âŒ {error_msg}")
            return {
                "question": "ç™¼ç”ŸéŒ¯èª¤",
                "options": ["éŒ¯èª¤", "éŒ¯èª¤", "éŒ¯èª¤", "éŒ¯èª¤"],
                "answer_index": 0,
                "explanation": error_msg
            }

    def chat_with_ai(self, message: str, chat_history: List[Dict] = None,
                    mode: str = "reference") -> str:
        """èˆ‡ AI å°è©±"""
        if chat_history is None:
            chat_history = []
        
        conversation_context = "\n".join([
            f"{'å­¸ç”Ÿ' if msg['role'] == 'user' else 'AIè€å¸«'}: {msg['content']}"
            for msg in chat_history[-5:]
        ])
        
        # åƒè€ƒæ›¸æ¨¡å¼ï¼šä½¿ç”¨çŸ¥è­˜åº«æœå°‹ç›¸é—œç­†è¨˜
        knowledge_context = ""
        if mode == "reference":
            try:
                # æœå°‹çŸ¥è­˜åº«ä¸­çš„ç›¸é—œç­†è¨˜
                search_results = self.search_knowledge_base(message, top_k=3)
                if search_results:
                    knowledge_context = "\n\nã€ç›¸é—œç­†è¨˜åƒè€ƒã€‘\n"
                    for i, result in enumerate(search_results, 1):
                        knowledge_context += f"\nåƒè€ƒè³‡æ–™ {i}ï¼š\n{result['content']}\n"
                    print(f"âœ… æ‰¾åˆ° {len(search_results)} å€‹ç›¸é—œç­†è¨˜")
                else:
                    print("âš ï¸ æ²’æœ‰æ‰¾åˆ°ç›¸é—œç­†è¨˜ï¼Œä½¿ç”¨ä¸€èˆ¬çŸ¥è­˜å›ç­”")
            except Exception as e:
                print(f"âš ï¸ çŸ¥è­˜åº«æœå°‹å¤±æ•—ï¼š{e}")
        
        mode_prompts = {
            "reference": f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ³•å¾‹åƒè€ƒæ›¸åŠ©æ‰‹ã€‚å­¸ç”Ÿæœƒå•ä½ å„ç¨®æ³•å¾‹å•é¡Œï¼Œè«‹ï¼š
- å„ªå…ˆåƒè€ƒä¸‹æ–¹çš„ã€ç›¸é—œç­†è¨˜åƒè€ƒã€‘ä¾†å›ç­”
- å¦‚æœç­†è¨˜ä¸­æœ‰ç›¸é—œå…§å®¹ï¼Œè«‹å¼•ç”¨ä¸¦èªªæ˜
- æä¾›æ¸…æ™°ã€æº–ç¢ºçš„ç­”æ¡ˆ
- å¼•ç”¨ç›¸é—œæ³•æ¢
- ç”¨æ˜“æ‡‚çš„æ–¹å¼è§£é‡‹è¤‡é›œæ¦‚å¿µ
- ä¿æŒå°ˆæ¥­ä½†å‹å–„çš„èªæ°£
- åªè¼¸å‡ºå›ç­”å…§å®¹ï¼Œä¸è¦æœ‰é–‹å ´ç™½æˆ–çµå°¾ç¥ç¦{knowledge_context}""",

            "socratic": """ä½ æ˜¯ä¸€ä½ä½¿ç”¨è˜‡æ ¼æ‹‰åº•å•ç­”æ³•çš„æ³•å¾‹è€å¸«ã€‚è«‹ï¼š
- å„ªå…ˆç”¨æå•å¼•å°å­¸ç”Ÿæ€è€ƒ
- å¦‚æœå­¸ç”Ÿæ˜ç¢ºè¦æ±‚ç›´æ¥ç­”æ¡ˆï¼ˆä¾‹å¦‚ï¼šã€Œç›´æ¥å‘Šè¨´æˆ‘ç­”æ¡ˆã€ã€ã€Œä¸è¦å•äº†ç›´æ¥èªªã€ï¼‰ï¼Œå°±ç›´æ¥çµ¦ç­”æ¡ˆ
- å¦‚æœå­¸ç”Ÿå·²ç¶“æ€è€ƒéä½†å¡ä½ï¼Œå¯ä»¥é©æ™‚çµ¦äºˆæç¤ºæˆ–ç­”æ¡ˆ
- å¾ªåºæ¼¸é€²åœ°æ·±å…¥æ¢è¨
- è®šè³å­¸ç”Ÿçš„æ€è€ƒéç¨‹
- ä¿æŒå½ˆæ€§ï¼Œæ ¹æ“šå­¸ç”Ÿçš„éœ€æ±‚èª¿æ•´æ•™å­¸æ–¹å¼
- åªè¼¸å‡ºå›ç­”å…§å®¹ï¼Œä¸è¦æœ‰é–‹å ´ç™½æˆ–çµå°¾ç¥ç¦""",

            "game": """ä½ æ˜¯ä¸€ä½å‡ºé¡Œæ¸¬é©—çš„æ³•å¾‹è€å¸«ã€‚è«‹ï¼š
- å‡ºå…·é«”çš„æ³•å¾‹çˆ­é»é¸æ“‡é¡Œæˆ–æƒ…å¢ƒé¡Œ
- æä¾›æ˜ç¢ºçš„é¸é …ï¼ˆA) B) C) D)ï¼‰
- å­¸ç”Ÿå›ç­”å¾Œçµ¦äºˆè©³ç´°è§£æ
- ä¿æŒéŠæˆ²åŒ–ã€æœ‰è¶£çš„æ°›åœ
- å¦‚æœå­¸ç”Ÿèªªã€Œé–‹å§‹ã€ï¼Œå°±å‡ºç¬¬ä¸€é¡Œ
- åªè¼¸å‡ºé¡Œç›®æˆ–è§£æå…§å®¹ï¼Œä¸è¦æœ‰é–‹å ´ç™½æˆ–çµå°¾ç¥ç¦"""
        }
        
        mode_instruction = mode_prompts.get(mode, mode_prompts["reference"])
        
        prompt = f"""{mode_instruction}

ã€å°è©±è¨˜éŒ„ã€‘
{conversation_context if conversation_context else 'ï¼ˆé€™æ˜¯å°è©±çš„é–‹å§‹ï¼‰'}

ã€å­¸ç”Ÿæœ€æ–°è¨Šæ¯ã€‘
{message}

è«‹å›æ‡‰å­¸ç”Ÿã€‚ç”¨ç¹é«”ä¸­æ–‡ã€‚"""
        
        try:
            print(f"ğŸ¤– {mode} æ¨¡å¼å›æ‡‰ä¸­...")
            response = self.model.generate_content(prompt)
            print(f"âœ… å›æ‡‰å®Œæˆ")
            return response.text
            
        except Exception as e:
            error_msg = f"å°è©±æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
            print(f"âŒ {error_msg}")
            return error_msg
    
    def generate_mind_map(self, content: str) -> str:
        """ç”Ÿæˆ Mermaid å¿ƒæ™ºåœ–"""
        prompt = f"""è«‹å°‡ä»¥ä¸‹ç­†è¨˜å…§å®¹è½‰æ›æˆ Mermaid å¿ƒæ™ºåœ–æ ¼å¼ã€‚

å…§å®¹ï¼š
{content}

åš´æ ¼è¦æ±‚ï¼š
1. ä½¿ç”¨ mindmap èªæ³•
2. æœ€å¤š3å±¤çµæ§‹
3. **æ‰€æœ‰æ–‡å­—å¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡**ï¼Œçµ•å°ä¸å¯ä»¥æœ‰è‹±æ–‡ã€å¾·æ–‡æˆ–å…¶ä»–èªè¨€
4. ç¯€é»åç¨±è¦ç°¡çŸ­ï¼ˆæ¯å€‹ç¯€é»ä¸è¶…é10å€‹å­—ï¼‰
5. é¿å…ä½¿ç”¨ç‰¹æ®Šç¬¦è™Ÿï¼Œåªç”¨ä¸­æ–‡ã€æ•¸å­—ã€æ‹¬è™Ÿ
6. åªè¼¸å‡º Mermaid ç¨‹å¼ç¢¼ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—æˆ–èªªæ˜
7. ä¸è¦ä½¿ç”¨ ``` ç¬¦è™ŸåŒ…è£¹

æ­£ç¢ºæ ¼å¼ç¯„ä¾‹ï¼š
mindmap
  root((ä¾µæ¬Šè¡Œç‚º))
    æ§‹æˆè¦ä»¶
      æ•…æ„æˆ–éå¤±
      ä¸æ³•è¡Œç‚º
      æå®³ç™¼ç”Ÿ
    æ³•å¾‹æ•ˆæœ
      æå®³è³ å„Ÿ
      å›å¾©åŸç‹€

è«‹ç”Ÿæˆï¼ˆåªè¼¸å‡ºç¨‹å¼ç¢¼ï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡‹ï¼‰ï¼š"""
        
        try:
            response = self.model.generate_content(prompt)
            result = response.text.strip()
            
            # ç§»é™¤å¯èƒ½çš„ markdown code block æ¨™è¨˜
            if "```" in result:
                parts = result.split("```")
                if len(parts) >= 3:
                    result = parts[1]
                elif len(parts) == 2:
                    result = parts[1]
            
            # ç§»é™¤èªè¨€æ¨™è¨˜
            if result.startswith("mermaid"):
                result = result[7:].strip()
            elif result.startswith("mindmap"):
                pass  # ä¿ç•™ mindmap
            
            # ç¢ºä¿ä»¥ mindmap é–‹é ­
            if not result.strip().startswith("mindmap"):
                result = "mindmap\n" + result
            
            return result.strip()
        except Exception as e:
            return f"mindmap\n  root((éŒ¯èª¤))\n    {str(e)}"
    
    def generate_legal_system_diagram(self, content: str) -> str:
        """ç”Ÿæˆæ³•å¾‹é«”ç³»æ¶æ§‹åœ–ï¼ˆç›´åˆ—ç‰ˆæœ¬ TBï¼‰"""
        prompt = f"""è«‹å°‡ä»¥ä¸‹æ³•å¾‹å…§å®¹è½‰æ›æˆé«”ç³»æ¶æ§‹åœ–ï¼ˆMermaid flowchartï¼‰ã€‚

å…§å®¹ï¼š
{content}

åš´æ ¼è¦æ±‚ï¼š
1. ä½¿ç”¨ flowchart TB èªæ³•ï¼ˆTop to Bottomï¼Œç”±ä¸Šè€Œä¸‹ç›´åˆ—æ’åˆ—ï¼‰
2. **æ‰€æœ‰æ–‡å­—å¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡**ï¼Œçµ•å°ä¸å¯ä»¥æœ‰è‹±æ–‡ã€å¾·æ–‡æˆ–å…¶ä»–èªè¨€
3. ç¯€é» ID ä½¿ç”¨ç°¡å–®çš„è‹±æ–‡å­—æ¯ï¼ˆA, B, C...ï¼‰
4. ç¯€é»æ¨™ç±¤ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œç”¨æ–¹æ‹¬è™ŸåŒ…è£¹ï¼Œä¾‹å¦‚ï¼šA[ä¾µæ¬Šè¡Œç‚º]
5. é€£æ¥ç·šä½¿ç”¨ --> ç¬¦è™Ÿ
6. ç¯€é»åç¨±è¦ç°¡çŸ­ï¼ˆæ¯å€‹ä¸è¶…é8å€‹å­—ï¼‰
7. åªè¼¸å‡º Mermaid ç¨‹å¼ç¢¼ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—æˆ–èªªæ˜
8. ä¸è¦ä½¿ç”¨ ``` ç¬¦è™ŸåŒ…è£¹

æ­£ç¢ºæ ¼å¼ç¯„ä¾‹ï¼š
flowchart TB
    A[ä¾µæ¬Šè¡Œç‚º] --> B[æ•…æ„ä¾µæ¬Š]
    A --> C[éå¤±ä¾µæ¬Š]
    B --> D[æå®³è³ å„Ÿ]
    C --> D

è«‹ç”Ÿæˆï¼ˆåªè¼¸å‡ºç¨‹å¼ç¢¼ï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡‹ï¼‰ï¼š"""
        
        try:
            response = self.model.generate_content(prompt)
            result = response.text.strip()
            
            # ç§»é™¤å¯èƒ½çš„ markdown code block æ¨™è¨˜
            if "```" in result:
                parts = result.split("```")
                if len(parts) >= 3:
                    result = parts[1]
                elif len(parts) == 2:
                    result = parts[1]
            
            # ç§»é™¤èªè¨€æ¨™è¨˜
            if result.startswith("mermaid"):
                result = result[7:].strip()
            elif result.startswith("flowchart"):
                pass  # ä¿ç•™ flowchart
            
            # ç¢ºä¿ä½¿ç”¨ TB æ–¹å‘ï¼ˆç›´åˆ—ï¼‰
            if not result.strip().startswith("flowchart TB"):
                result = result.replace("flowchart LR", "flowchart TB")
                result = result.replace("flowchart TD", "flowchart TB")
                if not result.strip().startswith("flowchart"):
                    result = "flowchart TB\n" + result
            
            return result.strip()
        except Exception as e:
            return f"flowchart TB\n    A[éŒ¯èª¤: {str(e)}]"
    
    def get_index_stats(self) -> Dict:
        """å–å¾—çŸ¥è­˜åº«çµ±è¨ˆè³‡è¨Š"""
        try:
            stats = self.index.describe_index_stats()
            return {
                'total_vectors': stats.get('total_vector_count', 0),
                'dimension': stats.get('dimension', Config.EMBEDDING_DIMENSION),
                'index_fullness': stats.get('index_fullness', 0)
            }
        except Exception as e:
            print(f"âŒ å–å¾—çµ±è¨ˆå¤±æ•—: {e}")
            return {
                'total_vectors': 0,
                'dimension': Config.EMBEDDING_DIMENSION,
                'index_fullness': 0
            }