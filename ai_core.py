"""
æ³•å¾‹è€ƒè©¦è¼”åŠ©ç³»çµ± - AI æ ¸å¿ƒæ¨¡çµ„ï¼ˆå®Œæ•´ç‰ˆ + é¢¨æ ¼æ”¯æ´ + è¦–è¦ºåŒ–ï¼‰
è™•ç† Gemini APIã€Pinecone å‘é‡æœå°‹ã€RAG å•ç­”ã€å¿ƒæ™ºåœ–ç”Ÿæˆ
"""

import google.generativeai as genai
from pinecone import Pinecone, ServerlessSpec
from config import Config
from ai_cache import AICache
import time
import json
import re
from typing import List, Dict, Optional

class AICore:
    """AI æ ¸å¿ƒè™•ç†é¡åˆ¥"""
    
    def __init__(self):
        """åˆå§‹åŒ– AI æœå‹™"""
        try:
            # é…ç½® Gemini
            genai.configure(api_key=Config.GEMINI_API_KEY)
            
            # åˆå§‹åŒ–æ¨¡å‹
            self.model = genai.GenerativeModel(Config.GEMINI_MODEL)
            self.embedding_model = Config.EMBEDDING_MODEL
            
            # åˆå§‹åŒ– Pinecone
            self.pc = Pinecone(api_key=Config.PINECONE_API_KEY)
            self.index = self._init_pinecone_index()
            
            # åˆå§‹åŒ– AI å¿«å–
            self.cache = AICache(cache_file="ai_cache.json", max_age_hours=24)
            
            print("âœ… AI æœå‹™åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ AI åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    def _init_pinecone_index(self):
        """åˆå§‹åŒ– Pinecone ç´¢å¼•"""
        index_name = Config.PINECONE_INDEX_NAME
        
        # æª¢æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        existing_indexes = [idx['name'] for idx in self.pc.list_indexes()]
        
        if index_name not in existing_indexes:
            print(f"ğŸ“¦ å»ºç«‹æ–°çš„ Pinecone ç´¢å¼•: {index_name}")
            self.pc.create_index(
                name=index_name,
                dimension=Config.EMBEDDING_DIMENSION,
                metric='cosine',
                spec=ServerlessSpec(
                    cloud='aws',
                    region='us-east-1'
                )
            )
            # ç­‰å¾…ç´¢å¼•æº–å‚™å°±ç·’
            print("â³ ç­‰å¾…ç´¢å¼•åˆå§‹åŒ–...")
            time.sleep(10)
        
        # é€£æ¥åˆ°ç´¢å¼•ä¸¦è¿”å›
        index = self.pc.Index(index_name)
        print(f"âœ… Pinecone ç´¢å¼•å·²é€£æ¥: {index_name}")
        return index
    
    def generate_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆæ–‡å­—åµŒå…¥å‘é‡"""
        try:
            result = genai.embed_content(
                model=Config.EMBEDDING_MODEL,
                content=text,
                task_type="retrieval_document"
            )
            return result['embedding']
        except Exception as e:
            print(f"âŒ åµŒå…¥ç”ŸæˆéŒ¯èª¤: {e}")
            return [0.0] * Config.EMBEDDING_DIMENSION
    
    def add_to_knowledge_base(self, content: str, metadata: Dict) -> bool:
        """æ–°å¢å…§å®¹åˆ°çŸ¥è­˜åº«"""
        try:
            embedding = self.generate_embedding(content)
            doc_id = f"{metadata.get('type', 'doc')}_{int(time.time() * 1000)}"
            
            self.index.upsert(
                vectors=[{
                    'id': doc_id,
                    'values': embedding,
                    'metadata': {
                        'content': content[:1000],
                        'full_content': content,
                        **metadata
                    }
                }]
            )
            
            print(f"âœ… å·²åŠ å…¥çŸ¥è­˜åº«: {metadata.get('title', 'Untitled')}")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ å…¥çŸ¥è­˜åº«å¤±æ•—: {e}")
            return False
    
    def delete_from_knowledge_base(self, note_id: str) -> bool:
        """å¾çŸ¥è­˜åº«åˆªé™¤å…§å®¹"""
        try:
            self.index.delete(ids=[note_id])
            print(f"âœ… å·²å¾çŸ¥è­˜åº«åˆªé™¤: {note_id}")
            return True
        except Exception as e:
            print(f"âŒ å¾çŸ¥è­˜åº«åˆªé™¤å¤±æ•—: {e}")
            return False
    
    def search_knowledge_base(self, query: str, top_k: int = None, 
                             category: str = None) -> List[Dict]:
        """æœå°‹çŸ¥è­˜åº«"""
        if top_k is None:
            top_k = Config.MAX_SEARCH_RESULTS
        
        try:
            query_embedding = self.generate_embedding(query)
            filter_dict = {'category': category} if category else None
            
            results = self.index.query(
                vector=query_embedding,
                top_k=top_k * 2,
                include_metadata=True,
                filter=filter_dict
            )
            
            # é™¤éŒ¯ï¼šé¡¯ç¤ºæ‰€æœ‰çµæœçš„åˆ†æ•¸
            print(f"ğŸ” Pinecone è¿”å› {len(results.get('matches', []))} å€‹çµæœ")
            for i, match in enumerate(results.get('matches', [])[:5]):
                print(f"  çµæœ {i+1}: åˆ†æ•¸ {match['score']:.3f}")
            
            filtered_results = []
            for match in results['matches']:
                # é–¾å€¼ 0.6ï¼šåªé¡¯ç¤ºé«˜åº¦ç›¸é—œçš„çµæœ
                if match['score'] >= 0.6:
                    filtered_results.append({
                        'score': match['score'],
                        'content': match['metadata'].get('full_content', 
                                                        match['metadata'].get('content', '')),
                        'metadata': match['metadata']
                    })
            
            filtered_results = filtered_results[:top_k]
            print(f"âœ… éæ¿¾å¾Œæ‰¾åˆ° {len(filtered_results)} å€‹ç›¸é—œçµæœï¼ˆé–¾å€¼ >= 0.6ï¼‰")
            return filtered_results
            
        except Exception as e:
            print(f"âŒ æœå°‹å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def generate_ai_notes(self, content: str, note_type: str = "é‡é»æ•´ç†", 
                         style_instruction: str = "") -> str:
        """ä½¿ç”¨ AI ç”Ÿæˆç­†è¨˜ï¼ˆæ”¯æ´è‡ªè¨‚é¢¨æ ¼ï¼‰"""
        
        base_prompts = {
            "é‡é»æ•´ç†": f"""è«‹å°‡ä»¥ä¸‹æ³•å¾‹å…§å®¹æ•´ç†æˆç­†è¨˜ã€‚

é¢¨æ ¼è¦æ±‚ï¼š{style_instruction}

åŸå§‹å…§å®¹ï¼š
{content}

**é‡è¦æŒ‡ç¤º**ï¼š
1. ç›´æ¥è¼¸å‡ºç­†è¨˜å…§å®¹ï¼Œä¸è¦æœ‰ä»»ä½•é–‹å ´ç™½ï¼ˆä¾‹å¦‚ï¼šã€Œå¥½çš„ã€ã€ã€Œä»¥ä¸‹æ˜¯ã€ã€ã€Œå·²ç‚ºæ‚¨æ•´ç†ã€ç­‰ï¼‰
2. ä¸è¦æœ‰çµå°¾ç¥ç¦æˆ–é¼“å‹µçš„è©±
3. ä¸è¦èªªæ˜ä½ åšäº†ä»€éº¼ï¼Œç›´æ¥çµ¦å‡ºçµæœ
4. ç”¨ç¹é«”ä¸­æ–‡
5. ç«‹å³å¾ç­†è¨˜æ¨™é¡Œæˆ–å…§å®¹é–‹å§‹""",

            "è€ƒé»åˆ†æ": f"""è«‹åˆ†æä»¥ä¸‹å…§å®¹çš„é‡è¦è€ƒé»ã€‚

é¢¨æ ¼è¦æ±‚ï¼š{style_instruction}

åŸå§‹å…§å®¹ï¼š
{content}

**é‡è¦æŒ‡ç¤º**ï¼š
1. ç›´æ¥è¼¸å‡ºè€ƒé»åˆ†æå…§å®¹ï¼Œä¸è¦æœ‰ä»»ä½•é–‹å ´ç™½ï¼ˆä¾‹å¦‚ï¼šã€Œå¥½çš„ã€ã€ã€Œä»¥ä¸‹æ˜¯ã€ã€ã€Œå·²ç‚ºæ‚¨æ•´ç†ã€ç­‰ï¼‰
2. ä¸è¦æœ‰çµå°¾ç¥ç¦æˆ–é¼“å‹µçš„è©±
3. ä¸è¦èªªæ˜ä½ åšäº†ä»€éº¼ï¼Œç›´æ¥çµ¦å‡ºçµæœ
4. ç”¨ç¹é«”ä¸­æ–‡
5. ç«‹å³å¾è€ƒé»å…§å®¹é–‹å§‹""",

            "æ¡ˆä¾‹è§£æ": f"""è«‹é‡å°ä»¥ä¸‹å…§å®¹æä¾›æ¡ˆä¾‹è§£æã€‚

é¢¨æ ¼è¦æ±‚ï¼š{style_instruction}

åŸå§‹å…§å®¹ï¼š
{content}

**é‡è¦æŒ‡ç¤º**ï¼š
1. ç›´æ¥è¼¸å‡ºæ¡ˆä¾‹è§£æå…§å®¹ï¼Œä¸è¦æœ‰ä»»ä½•é–‹å ´ç™½ï¼ˆä¾‹å¦‚ï¼šã€Œå¥½çš„ã€ã€ã€Œä»¥ä¸‹æ˜¯ã€ã€ã€Œå·²ç‚ºæ‚¨æ•´ç†ã€ç­‰ï¼‰
2. ä¸è¦æœ‰çµå°¾ç¥ç¦æˆ–é¼“å‹µçš„è©±
3. ä¸è¦èªªæ˜ä½ åšäº†ä»€éº¼ï¼Œç›´æ¥çµ¦å‡ºçµæœ
4. ç”¨ç¹é«”ä¸­æ–‡
5. ç«‹å³å¾æ¡ˆä¾‹å…§å®¹é–‹å§‹"""
        }
        
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                prompt = base_prompts.get(note_type, base_prompts["é‡é»æ•´ç†"])
                print(f"ğŸ¤– æ­£åœ¨ç”Ÿæˆ{note_type}... (å˜—è©¦ {attempt + 1}/{max_retries})")
                
                # ç”Ÿæˆå…§å®¹ï¼ˆGemini API ä¸æ”¯æ´ request_optionsï¼‰
                response = self.model.generate_content(prompt)
                
                print(f"âœ… {note_type}ç”Ÿæˆå®Œæˆ")
                
                # å¾Œè™•ç†ï¼šç§»é™¤å¸¸è¦‹çš„é–‹å ´ç™½æ¨¡å¼
                result = response.text.strip()
                
                # ç§»é™¤å¸¸è¦‹çš„é–‹å ´ç™½å¥å­ï¼ˆåŒ…å« OCR å°ˆç”¨ï¼‰
                preamble_patterns = [
                    r'^å¥½çš„[ï¼Œ,ã€‚ï¼!]*',
                    r'^ä»¥ä¸‹æ˜¯.*?[ï¼š:]\s*',
                    r'^å·²ç‚ºæ‚¨æ•´ç†.*?[ï¼š:]\s*',
                    r'^é€™ä»½.*?ç­†è¨˜.*?å¦‚ä¸‹.*?[ï¼š:]\s*',
                    r'^æ ¹æ“š.*?å…§å®¹.*?[ï¼š:]\s*',
                    r'^.*?æ•´ç†å¦‚ä¸‹.*?[ï¼š:]\s*',
                    r'^.*?å·²æ•´ç†.*?[ï¼š:]\s*',
                    r'^è®“æˆ‘.*?[ï¼š:]\s*',
                    r'^æˆ‘å°‡.*?[ï¼š:]\s*',
                    r'^ä»¥ä¸‹.*?Markdown.*?[ï¼š:]\s*',
                    # OCR å°ˆç”¨æ¨¡å¼
                    r'^ä»¥ä¸‹æ˜¯æ ¹æ“š.*?OCR.*?è¾¨è­˜.*?[ï¼š:ï¼Œ,ã€‚]*\s*',
                    r'^æ ¹æ“š.*?OCR.*?æ–‡å­—.*?[ï¼š:ï¼Œ,ã€‚]*\s*',
                    r'^ä»¥ä¸‹æ˜¯.*?OCR.*?å…§å®¹.*?[ï¼š:ï¼Œ,ã€‚]*\s*',
                    r'^.*?æ•´ç†è€Œæˆçš„.*?å…§å®¹.*?[ï¼š:ï¼Œ,ã€‚]*\s*',
                    r'^.*?é€šé †.*?å®Œæ•´.*?æ ¼å¼åŒ–.*?[ï¼š:ï¼Œ,ã€‚]*\s*',
                ]
                
                for pattern in preamble_patterns:
                    result = re.sub(pattern, '', result, flags=re.MULTILINE)
                
                # ç§»é™¤é–‹é ­çš„ç©ºè¡Œ
                result = result.lstrip('\n')
                
                return result
                
            except Exception as e:
                error_str = str(e)
                if "504" in error_str or "timeout" in error_str.lower():
                    if attempt < max_retries - 1:
                        print(f"âš ï¸ è«‹æ±‚è¶…æ™‚ï¼Œ{retry_delay}ç§’å¾Œé‡è©¦...")
                        time.sleep(retry_delay)
                        retry_delay *= 2  # æŒ‡æ•¸é€€é¿
                        continue
                    else:
                        return f"ç”Ÿæˆç­†è¨˜æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼šè«‹æ±‚è¶…æ™‚ã€‚è«‹å˜—è©¦ï¼š\n1. æ¸›å°‘è¼¸å…¥å…§å®¹é•·åº¦\n2. ç¨å¾Œå†è©¦\n3. æª¢æŸ¥ç¶²è·¯é€£ç·š"
                else:
                    error_msg = f"ç”Ÿæˆç­†è¨˜æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
                    print(f"âŒ {error_msg}")
                    return error_msg
    
    def answer_question_with_rag(self, question: str, 
                                 context_results: List[Dict] = None) -> str:
        """ä½¿ç”¨ RAG å›ç­”å•é¡Œ"""
        if context_results is None:
            context_results = self.search_knowledge_base(question, top_k=3)
        
        if context_results:
            context = "\n\n".join([
                f"ã€åƒè€ƒè³‡æ–™ {i+1}ã€‘ï¼ˆç›¸é—œåº¦: {r['score']:.1%})\n"
                f"æ¨™é¡Œ: {r['metadata'].get('title', 'ç„¡æ¨™é¡Œ')}\n"
                f"å…§å®¹: {r['content']}"
                for i, r in enumerate(context_results)
            ])
        else:
            context = "ï¼ˆçŸ¥è­˜åº«ä¸­æš«ç„¡ç›¸é—œè³‡æ–™ï¼‰"
        
        prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ³•å¾‹è€ƒè©¦è¼”å°è€å¸«ã€‚è«‹æ ¹æ“šçŸ¥è­˜åº«å…§å®¹å›ç­”å­¸ç”Ÿçš„å•é¡Œã€‚

ã€çŸ¥è­˜åº«åƒè€ƒè³‡æ–™ã€‘
{context}

ã€å­¸ç”Ÿå•é¡Œã€‘
{question}

è«‹ä»¥å°ˆæ¥­ä½†æ˜“æ‡‚çš„æ–¹å¼å›ç­”ï¼ŒåŒ…å«ï¼š
1. ç›´æ¥å›ç­”å•é¡Œçš„æ ¸å¿ƒ
2. å¼•ç”¨ç›¸é—œæ³•æ¢ï¼ˆå¦‚æœæœ‰ï¼‰
3. èªªæ˜åœ¨å¯¦å‹™ä¸Šçš„æ‡‰ç”¨
4. è£œå……æ³¨æ„äº‹é …

å¦‚æœçŸ¥è­˜åº«å…§å®¹ä¸è¶³ï¼Œè«‹å…ˆèªªæ˜ã€ŒçŸ¥è­˜åº«ä¸­ç›¸é—œè³‡æ–™æœ‰é™ã€ï¼Œç„¶å¾ŒåŸºæ–¼ä¸€èˆ¬æ³•å¾‹åŸå‰‡æä¾›å»ºè­°ã€‚
åªè¼¸å‡ºå›ç­”å…§å®¹ï¼Œä¸è¦æœ‰é–‹å ´ç™½æˆ–çµå°¾ç¥ç¦ã€‚ç”¨ç¹é«”ä¸­æ–‡ã€‚"""
        
        try:
            print(f"ğŸ¤– æ­£åœ¨æ€è€ƒç­”æ¡ˆ...")
            response = self.model.generate_content(prompt)
            print(f"âœ… å›ç­”å®Œæˆ")
            return response.text
            
        except Exception as e:
            error_msg = f"å›ç­”å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
            print(f"âŒ {error_msg}")
            return error_msg
    
    def generate_quiz_question(self, content: str = None, category: str = None) -> Dict:
        """ç”Ÿæˆæ³•å¾‹çˆ­é»é¸æ“‡é¡Œï¼ˆJSON æ ¼å¼ï¼‰"""
        
        source_content = ""
        if content:
            source_content = f"åŸºæ–¼ä»¥ä¸‹ç­†è¨˜å…§å®¹å‡ºé¡Œï¼š\n{content}"
        else:
            source_content = f"è«‹å‡ºä¸€é¡Œé—œæ–¼ã€Œ{category if category else 'æ³•å¾‹'}ã€çš„çˆ­é»é¸æ“‡é¡Œã€‚"

        prompt = f"""ä½ æ˜¯ä¸€å€‹æ³•å¾‹å‡ºé¡Œè€å¸«ã€‚è«‹{source_content}

è«‹ç”Ÿæˆä¸€å€‹å–®é¸é¡Œï¼Œæ ¼å¼å¿…é ˆæ˜¯åˆæ³•çš„ JSONï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
1. question: é¡Œç›®æ•˜è¿°
2. options: ä¸€å€‹åŒ…å« 4 å€‹é¸é …å­—ä¸²çš„é™£åˆ— (ä¸éœ€è¦ A, B, C, D å‰ç¶´)
3. answer_index: æ­£ç¢ºç­”æ¡ˆçš„ç´¢å¼• (0-3)
4. explanation: è©³ç´°è§£æ

ç¯„ä¾‹æ ¼å¼ï¼š
{{
  "question": "é—œæ–¼...ä¸‹åˆ—ä½•è€…æ­£ç¢ºï¼Ÿ",
  "options": ["é¸é …ä¸€", "é¸é …äºŒ", "é¸é …ä¸‰", "é¸é …å››"],
  "answer_index": 1,
  "explanation": "å› ç‚º..."
}}

è«‹åªè¼¸å‡º JSONï¼Œä¸è¦æœ‰ Markdown code block æ¨™è¨˜æˆ–å…¶ä»–æ–‡å­—ã€‚ç”¨ç¹é«”ä¸­æ–‡ã€‚"""

        try:
            print(f"ğŸ¤– æ­£åœ¨ç”Ÿæˆé¡Œç›®...")
            response = self.model.generate_content(prompt)
            text = response.text.strip()
            
            # å˜—è©¦è§£æ JSON
            match = re.search(r'\{.*\}', text, re.DOTALL)
            if match:
                json_str = match.group(0)
                quiz_data = json.loads(json_str)
                print(f"âœ… é¡Œç›®ç”Ÿæˆå®Œæˆ")
                return quiz_data
            else:
                print(f"âš ï¸ ç„¡æ³•è§£æ JSONï¼ŒåŸå§‹å›æ‡‰ï¼š{text}")
                return {
                    "question": "é¡Œç›®ç”Ÿæˆå¤±æ•—ï¼Œè«‹é‡è©¦",
                    "options": ["éŒ¯èª¤", "éŒ¯èª¤", "éŒ¯èª¤", "éŒ¯èª¤"],
                    "answer_index": 0,
                    "explanation": f"ç„¡æ³•è§£æ AI å›æ‡‰: {text}"
                }
                
        except Exception as e:
            error_msg = f"ç”Ÿæˆé¡Œç›®éŒ¯èª¤: {e}"
            print(f"âŒ {error_msg}")
            return {
                "question": "ç™¼ç”ŸéŒ¯èª¤",
                "options": ["éŒ¯èª¤", "éŒ¯èª¤", "éŒ¯èª¤", "éŒ¯èª¤"],
                "answer_index": 0,
                "explanation": error_msg
            }

    def chat_with_ai(self, message: str, chat_history: List[Dict] = None,
                    mode: str = "reference") -> str:
        """èˆ‡ AI å°è©±"""
        if chat_history is None:
            chat_history = []
        
        conversation_context = "\n".join([
            f"{'å­¸ç”Ÿ' if msg['role'] == 'user' else 'AIè€å¸«'}: {msg['content']}"
            for msg in chat_history[-5:]
        ])
        
        mode_prompts = {
            "reference": """ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ³•å¾‹åƒè€ƒæ›¸åŠ©æ‰‹ã€‚å­¸ç”Ÿæœƒå•ä½ å„ç¨®æ³•å¾‹å•é¡Œï¼Œè«‹ï¼š
- æä¾›æ¸…æ™°ã€æº–ç¢ºçš„ç­”æ¡ˆ
- å¼•ç”¨ç›¸é—œæ³•æ¢
- ç”¨æ˜“æ‡‚çš„æ–¹å¼è§£é‡‹è¤‡é›œæ¦‚å¿µ
- ä¿æŒå°ˆæ¥­ä½†å‹å–„çš„èªæ°£
- åªè¼¸å‡ºå›ç­”å…§å®¹ï¼Œä¸è¦æœ‰é–‹å ´ç™½æˆ–çµå°¾ç¥ç¦""",

            "socratic": """ä½ æ˜¯ä¸€ä½ä½¿ç”¨è˜‡æ ¼æ‹‰åº•å•ç­”æ³•çš„æ³•å¾‹è€å¸«ã€‚è«‹ï¼š
- ç”¨æå•å¼•å°å­¸ç”Ÿæ€è€ƒ
- ä¸è¦ç›´æ¥çµ¦ç­”æ¡ˆï¼Œè€Œæ˜¯å¼•å°å­¸ç”Ÿè‡ªå·±æ‰¾åˆ°ç­”æ¡ˆ
- å¾ªåºæ¼¸é€²åœ°æ·±å…¥æ¢è¨
- è®šè³å­¸ç”Ÿçš„æ€è€ƒéç¨‹
- åªè¼¸å‡ºå›ç­”å…§å®¹ï¼Œä¸è¦æœ‰é–‹å ´ç™½æˆ–çµå°¾ç¥ç¦""",

            "game": """ä½ æ˜¯ä¸€ä½å‡ºé¡Œæ¸¬é©—çš„æ³•å¾‹è€å¸«ã€‚è«‹ï¼š
- å‡ºå…·é«”çš„æ³•å¾‹çˆ­é»é¸æ“‡é¡Œæˆ–æƒ…å¢ƒé¡Œ
- æä¾›æ˜ç¢ºçš„é¸é …ï¼ˆA) B) C) D)ï¼‰
- å­¸ç”Ÿå›ç­”å¾Œçµ¦äºˆè©³ç´°è§£æ
- ä¿æŒéŠæˆ²åŒ–ã€æœ‰è¶£çš„æ°›åœ
- å¦‚æœå­¸ç”Ÿèªªã€Œé–‹å§‹ã€ï¼Œå°±å‡ºç¬¬ä¸€é¡Œ
- åªè¼¸å‡ºé¡Œç›®æˆ–è§£æå…§å®¹ï¼Œä¸è¦æœ‰é–‹å ´ç™½æˆ–çµå°¾ç¥ç¦"""
        }
        
        mode_instruction = mode_prompts.get(mode, mode_prompts["reference"])
        
        prompt = f"""{mode_instruction}

ã€å°è©±è¨˜éŒ„ã€‘
{conversation_context if conversation_context else 'ï¼ˆé€™æ˜¯å°è©±çš„é–‹å§‹ï¼‰'}

ã€å­¸ç”Ÿæœ€æ–°è¨Šæ¯ã€‘
{message}

è«‹å›æ‡‰å­¸ç”Ÿã€‚ç”¨ç¹é«”ä¸­æ–‡ã€‚"""
        
        try:
            print(f"ğŸ¤– {mode} æ¨¡å¼å›æ‡‰ä¸­...")
            response = self.model.generate_content(prompt)
            print(f"âœ… å›æ‡‰å®Œæˆ")
            return response.text
            
        except Exception as e:
            error_msg = f"å°è©±æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
            print(f"âŒ {error_msg}")
            return error_msg
    
    def generate_mind_map(self, content: str) -> str:
        """ç”Ÿæˆ Mermaid å¿ƒæ™ºåœ–"""
        prompt = f"""è«‹å°‡ä»¥ä¸‹ç­†è¨˜å…§å®¹è½‰æ›æˆ Mermaid å¿ƒæ™ºåœ–æ ¼å¼ã€‚

å…§å®¹ï¼š
{content}

åš´æ ¼è¦æ±‚ï¼š
1. ä½¿ç”¨ mindmap èªæ³•
2. æœ€å¤š3å±¤çµæ§‹
3. **æ‰€æœ‰æ–‡å­—å¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡**ï¼Œçµ•å°ä¸å¯ä»¥æœ‰è‹±æ–‡ã€å¾·æ–‡æˆ–å…¶ä»–èªè¨€
4. ç¯€é»åç¨±è¦ç°¡çŸ­ï¼ˆæ¯å€‹ç¯€é»ä¸è¶…é10å€‹å­—ï¼‰
5. é¿å…ä½¿ç”¨ç‰¹æ®Šç¬¦è™Ÿï¼Œåªç”¨ä¸­æ–‡ã€æ•¸å­—ã€æ‹¬è™Ÿ
6. åªè¼¸å‡º Mermaid ç¨‹å¼ç¢¼ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—æˆ–èªªæ˜
7. ä¸è¦ä½¿ç”¨ ``` ç¬¦è™ŸåŒ…è£¹

æ­£ç¢ºæ ¼å¼ç¯„ä¾‹ï¼š
mindmap
  root((ä¾µæ¬Šè¡Œç‚º))
    æ§‹æˆè¦ä»¶
      æ•…æ„æˆ–éå¤±
      ä¸æ³•è¡Œç‚º
      æå®³ç™¼ç”Ÿ
    æ³•å¾‹æ•ˆæœ
      æå®³è³ å„Ÿ
      å›å¾©åŸç‹€

è«‹ç”Ÿæˆï¼ˆåªè¼¸å‡ºç¨‹å¼ç¢¼ï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡‹ï¼‰ï¼š"""
        
        try:
            response = self.model.generate_content(prompt)
            result = response.text.strip()
            
            # ç§»é™¤å¯èƒ½çš„ markdown code block æ¨™è¨˜
            if "```" in result:
                parts = result.split("```")
                if len(parts) >= 3:
                    result = parts[1]
                elif len(parts) == 2:
                    result = parts[1]
            
            # ç§»é™¤èªè¨€æ¨™è¨˜
            if result.startswith("mermaid"):
                result = result[7:].strip()
            elif result.startswith("mindmap"):
                pass  # ä¿ç•™ mindmap
            
            # ç¢ºä¿ä»¥ mindmap é–‹é ­
            if not result.strip().startswith("mindmap"):
                result = "mindmap\n" + result
            
            return result.strip()
        except Exception as e:
            return f"mindmap\n  root((éŒ¯èª¤))\n    {str(e)}"
    
    def generate_legal_system_diagram(self, content: str) -> str:
        """ç”Ÿæˆæ³•å¾‹é«”ç³»æ¶æ§‹åœ–ï¼ˆç›´åˆ—ç‰ˆæœ¬ TBï¼‰"""
        prompt = f"""è«‹å°‡ä»¥ä¸‹æ³•å¾‹å…§å®¹è½‰æ›æˆé«”ç³»æ¶æ§‹åœ–ï¼ˆMermaid flowchartï¼‰ã€‚

å…§å®¹ï¼š
{content}

åš´æ ¼è¦æ±‚ï¼š
1. ä½¿ç”¨ flowchart TB èªæ³•ï¼ˆTop to Bottomï¼Œç”±ä¸Šè€Œä¸‹ç›´åˆ—æ’åˆ—ï¼‰
2. **æ‰€æœ‰æ–‡å­—å¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡**ï¼Œçµ•å°ä¸å¯ä»¥æœ‰è‹±æ–‡ã€å¾·æ–‡æˆ–å…¶ä»–èªè¨€
3. ç¯€é» ID ä½¿ç”¨ç°¡å–®çš„è‹±æ–‡å­—æ¯ï¼ˆA, B, C...ï¼‰
4. ç¯€é»æ¨™ç±¤ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œç”¨æ–¹æ‹¬è™ŸåŒ…è£¹ï¼Œä¾‹å¦‚ï¼šA[ä¾µæ¬Šè¡Œç‚º]
5. é€£æ¥ç·šä½¿ç”¨ --> ç¬¦è™Ÿ
6. ç¯€é»åç¨±è¦ç°¡çŸ­ï¼ˆæ¯å€‹ä¸è¶…é8å€‹å­—ï¼‰
7. åªè¼¸å‡º Mermaid ç¨‹å¼ç¢¼ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—æˆ–èªªæ˜
8. ä¸è¦ä½¿ç”¨ ``` ç¬¦è™ŸåŒ…è£¹

æ­£ç¢ºæ ¼å¼ç¯„ä¾‹ï¼š
flowchart TB
    A[ä¾µæ¬Šè¡Œç‚º] --> B[æ•…æ„ä¾µæ¬Š]
    A --> C[éå¤±ä¾µæ¬Š]
    B --> D[æå®³è³ å„Ÿ]
    C --> D

è«‹ç”Ÿæˆï¼ˆåªè¼¸å‡ºç¨‹å¼ç¢¼ï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡‹ï¼‰ï¼š"""
        
        try:
            response = self.model.generate_content(prompt)
            result = response.text.strip()
            
            # ç§»é™¤å¯èƒ½çš„ markdown code block æ¨™è¨˜
            if "```" in result:
                parts = result.split("```")
                if len(parts) >= 3:
                    result = parts[1]
                elif len(parts) == 2:
                    result = parts[1]
            
            # ç§»é™¤èªè¨€æ¨™è¨˜
            if result.startswith("mermaid"):
                result = result[7:].strip()
            elif result.startswith("flowchart"):
                pass  # ä¿ç•™ flowchart
            
            # ç¢ºä¿ä½¿ç”¨ TB æ–¹å‘ï¼ˆç›´åˆ—ï¼‰
            if not result.strip().startswith("flowchart TB"):
                result = result.replace("flowchart LR", "flowchart TB")
                result = result.replace("flowchart TD", "flowchart TB")
                if not result.strip().startswith("flowchart"):
                    result = "flowchart TB\n" + result
            
            return result.strip()
        except Exception as e:
            return f"flowchart TB\n    A[éŒ¯èª¤: {str(e)}]"
    
    def get_index_stats(self) -> Dict:
        """å–å¾—çŸ¥è­˜åº«çµ±è¨ˆè³‡è¨Š"""
        try:
            stats = self.index.describe_index_stats()
            return {
                'total_vectors': stats.get('total_vector_count', 0),
                'dimension': stats.get('dimension', Config.EMBEDDING_DIMENSION),
                'index_fullness': stats.get('index_fullness', 0)
            }
        except Exception as e:
            print(f"âŒ å–å¾—çµ±è¨ˆå¤±æ•—: {e}")
            return {
                'total_vectors': 0,
                'dimension': Config.EMBEDDING_DIMENSION,
                'index_fullness': 0
            }